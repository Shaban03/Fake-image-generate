{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaban03/Fake-image-generate/blob/main/Fake_Image_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE2sPGj2oSJd"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJyokA52-MTI",
        "outputId": "b379fe05-dc28-4ae7-c997-a4941f7ee06b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name=\"/content/abc/cifar-10-python.tar.zip\""
      ],
      "metadata": {
        "id": "_CRmKyWeAXuE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlWbCO_jDc3t",
        "outputId": "0f95813a-327a-4c8c-cdab-08da4355c300"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "piEXOC22DdmN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting some hyperparameters\n",
        "batchSize = 64 # We set the size of the batch.\n",
        "imageSize = 64 # We set the size of the generated images (64x64)."
      ],
      "metadata": {
        "id": "AUDr0Ua6BSlz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the transformations\n",
        "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
      ],
      "metadata": {
        "id": "rx-tJM1TBZU3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dset.CIFAR10(root = '/content/abc/cifar-10-python.tar.zip', download = True, transform = transform)  #load dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38kb7sLJBxVQ",
        "outputId": "99d0ae5c-7165-459f-e2ee-9440cc62965d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/abc/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 68428900.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/abc/cifar-10-python.tar.gz to /content/abc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "id": "3zESmc38CTuG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "metadata": {
        "id": "K8eALOr5CxNN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class G(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(G, self).__init__() # We inherit from the nn.Module tools.\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            #Output as of 3 channel...\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output # We return the output containing the generated images.\n"
      ],
      "metadata": {
        "id": "qvmxvqXnC6Kr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the generator\n",
        "netG = G()\n",
        "netG.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWzzeluVEQ4d",
        "outputId": "cd60b66e-34c5-4905-d646-e06e500db6e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)"
      ],
      "metadata": {
        "id": "_EHyuEVqEYHf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the discriminator\n",
        "netD = D()\n",
        "netD.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta00XQFcFGcU",
        "outputId": "f8d575ab-3be3-4101-b812-ee50d7410ddf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "D(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the DCGANs\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "\n",
        "for epoch in range(25):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        #Firstly, updating the weights of the neural network of the discriminator\n",
        "        netD.zero_grad()\n",
        "        # Training the discriminator with a real image of the dataset\n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "\n",
        "        # Training the discriminator with a fake image generated by the generator\n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "\n",
        "        # Backpropagating the total error\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        #Secondly, Updating the weights of the neural network of the generator\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Third, printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data, errG.data))\n",
        "        if i % 100 == 0:\n",
        "            vutils.save_image(real, '%s/real_samples.png' % \"/content/drive/My Drive/abc/result\", normalize = True)\n",
        "            fake = netG(noise)\n",
        "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"/content/drive/My Drive/abc/result\", epoch), normalize = True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jea08lsFO1S",
        "outputId": "a23e54ed-253b-4c45-bce9-5b354aa97287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/25][0/782] Loss_D: 1.5700 Loss_G: 5.2908\n",
            "[0/25][1/782] Loss_D: 1.0505 Loss_G: 6.7742\n",
            "[0/25][2/782] Loss_D: 0.6952 Loss_G: 5.9655\n",
            "[0/25][3/782] Loss_D: 0.9628 Loss_G: 6.6090\n",
            "[0/25][4/782] Loss_D: 1.1716 Loss_G: 6.2219\n",
            "[0/25][5/782] Loss_D: 0.8128 Loss_G: 7.5488\n",
            "[0/25][6/782] Loss_D: 0.6063 Loss_G: 6.9521\n",
            "[0/25][7/782] Loss_D: 0.9630 Loss_G: 7.7239\n",
            "[0/25][8/782] Loss_D: 0.4317 Loss_G: 8.1632\n",
            "[0/25][9/782] Loss_D: 0.6800 Loss_G: 8.8418\n",
            "[0/25][10/782] Loss_D: 0.5769 Loss_G: 8.7753\n",
            "[0/25][11/782] Loss_D: 0.5497 Loss_G: 8.6692\n",
            "[0/25][12/782] Loss_D: 0.9770 Loss_G: 12.4710\n",
            "[0/25][13/782] Loss_D: 0.5752 Loss_G: 8.3002\n",
            "[0/25][14/782] Loss_D: 1.4200 Loss_G: 14.7474\n",
            "[0/25][15/782] Loss_D: 0.3425 Loss_G: 12.9240\n",
            "[0/25][16/782] Loss_D: 0.1806 Loss_G: 7.0220\n",
            "[0/25][17/782] Loss_D: 2.4546 Loss_G: 15.3816\n",
            "[0/25][18/782] Loss_D: 0.2950 Loss_G: 14.9573\n",
            "[0/25][19/782] Loss_D: 0.5728 Loss_G: 8.5943\n",
            "[0/25][20/782] Loss_D: 1.8922 Loss_G: 15.1819\n",
            "[0/25][21/782] Loss_D: 0.2733 Loss_G: 14.4661\n",
            "[0/25][22/782] Loss_D: 0.4121 Loss_G: 7.4344\n",
            "[0/25][23/782] Loss_D: 3.4800 Loss_G: 16.8702\n",
            "[0/25][24/782] Loss_D: 0.3815 Loss_G: 17.0508\n",
            "[0/25][25/782] Loss_D: 0.2959 Loss_G: 11.9585\n",
            "[0/25][26/782] Loss_D: 0.2755 Loss_G: 6.9867\n",
            "[0/25][27/782] Loss_D: 2.2266 Loss_G: 19.2770\n",
            "[0/25][28/782] Loss_D: 0.4701 Loss_G: 21.0898\n",
            "[0/25][29/782] Loss_D: 0.3342 Loss_G: 17.1225\n",
            "[0/25][30/782] Loss_D: 0.1189 Loss_G: 9.5774\n",
            "[0/25][31/782] Loss_D: 0.4000 Loss_G: 10.6334\n",
            "[0/25][32/782] Loss_D: 0.2309 Loss_G: 10.8150\n",
            "[0/25][33/782] Loss_D: 0.1849 Loss_G: 9.5965\n",
            "[0/25][34/782] Loss_D: 0.5314 Loss_G: 14.2885\n",
            "[0/25][35/782] Loss_D: 0.3744 Loss_G: 11.2223\n",
            "[0/25][36/782] Loss_D: 0.2993 Loss_G: 7.6698\n",
            "[0/25][37/782] Loss_D: 1.2492 Loss_G: 21.5790\n",
            "[0/25][38/782] Loss_D: 0.3765 Loss_G: 23.9102\n",
            "[0/25][39/782] Loss_D: 0.3469 Loss_G: 20.2711\n",
            "[0/25][40/782] Loss_D: 0.0814 Loss_G: 12.5504\n",
            "[0/25][41/782] Loss_D: 0.2220 Loss_G: 8.9277\n",
            "[0/25][42/782] Loss_D: 0.9746 Loss_G: 21.9474\n",
            "[0/25][43/782] Loss_D: 0.3166 Loss_G: 24.1950\n",
            "[0/25][44/782] Loss_D: 0.1592 Loss_G: 21.3139\n",
            "[0/25][45/782] Loss_D: 0.2265 Loss_G: 14.3212\n",
            "[0/25][46/782] Loss_D: 0.0556 Loss_G: 6.0563\n",
            "[0/25][47/782] Loss_D: 2.0112 Loss_G: 24.2296\n",
            "[0/25][48/782] Loss_D: 0.2738 Loss_G: 28.1913\n",
            "[0/25][49/782] Loss_D: 0.4178 Loss_G: 27.5632\n",
            "[0/25][50/782] Loss_D: 0.2710 Loss_G: 23.9850\n",
            "[0/25][51/782] Loss_D: 0.0431 Loss_G: 16.9694\n",
            "[0/25][52/782] Loss_D: 0.0307 Loss_G: 8.3861\n",
            "[0/25][53/782] Loss_D: 0.4021 Loss_G: 13.2251\n",
            "[0/25][54/782] Loss_D: 0.0251 Loss_G: 13.5801\n",
            "[0/25][55/782] Loss_D: 0.0659 Loss_G: 9.9474\n",
            "[0/25][56/782] Loss_D: 0.1396 Loss_G: 6.5016\n",
            "[0/25][57/782] Loss_D: 0.6960 Loss_G: 18.3533\n",
            "[0/25][58/782] Loss_D: 0.6522 Loss_G: 18.5383\n",
            "[0/25][59/782] Loss_D: 0.2798 Loss_G: 13.5857\n",
            "[0/25][60/782] Loss_D: 0.0371 Loss_G: 6.8457\n",
            "[0/25][61/782] Loss_D: 1.6794 Loss_G: 24.9789\n",
            "[0/25][62/782] Loss_D: 0.2313 Loss_G: 28.9823\n",
            "[0/25][63/782] Loss_D: 0.4884 Loss_G: 28.7340\n",
            "[0/25][64/782] Loss_D: 0.1923 Loss_G: 26.4771\n",
            "[0/25][65/782] Loss_D: 0.0812 Loss_G: 22.5780\n",
            "[0/25][66/782] Loss_D: 0.0447 Loss_G: 16.3136\n",
            "[0/25][67/782] Loss_D: 0.0137 Loss_G: 9.4078\n",
            "[0/25][68/782] Loss_D: 0.1557 Loss_G: 7.7800\n",
            "[0/25][69/782] Loss_D: 0.4395 Loss_G: 18.3430\n",
            "[0/25][70/782] Loss_D: 0.2539 Loss_G: 18.8026\n",
            "[0/25][71/782] Loss_D: 0.2297 Loss_G: 12.6351\n",
            "[0/25][72/782] Loss_D: 0.2049 Loss_G: 7.6620\n",
            "[0/25][73/782] Loss_D: 1.5019 Loss_G: 26.7828\n",
            "[0/25][74/782] Loss_D: 2.2745 Loss_G: 28.4919\n",
            "[0/25][75/782] Loss_D: 1.1332 Loss_G: 25.5701\n",
            "[0/25][76/782] Loss_D: 0.0183 Loss_G: 20.3688\n",
            "[0/25][77/782] Loss_D: 0.0194 Loss_G: 12.8188\n",
            "[0/25][78/782] Loss_D: 0.0788 Loss_G: 4.5220\n",
            "[0/25][79/782] Loss_D: 3.0023 Loss_G: 25.8675\n",
            "[0/25][80/782] Loss_D: 1.3783 Loss_G: 28.3451\n",
            "[0/25][81/782] Loss_D: 0.6199 Loss_G: 26.9027\n",
            "[0/25][82/782] Loss_D: 0.1544 Loss_G: 22.9606\n",
            "[0/25][83/782] Loss_D: 0.2543 Loss_G: 17.7258\n",
            "[0/25][84/782] Loss_D: 0.0172 Loss_G: 10.7504\n",
            "[0/25][85/782] Loss_D: 0.0829 Loss_G: 5.0421\n",
            "[0/25][86/782] Loss_D: 1.5378 Loss_G: 21.1175\n",
            "[0/25][87/782] Loss_D: 0.6450 Loss_G: 23.3067\n",
            "[0/25][88/782] Loss_D: 0.4558 Loss_G: 20.7058\n",
            "[0/25][89/782] Loss_D: 0.2421 Loss_G: 15.0189\n",
            "[0/25][90/782] Loss_D: 0.1939 Loss_G: 9.6801\n",
            "[0/25][91/782] Loss_D: 0.1450 Loss_G: 5.3630\n",
            "[0/25][92/782] Loss_D: 0.7269 Loss_G: 10.0742\n",
            "[0/25][93/782] Loss_D: 0.2483 Loss_G: 10.5701\n",
            "[0/25][94/782] Loss_D: 0.1510 Loss_G: 6.4905\n",
            "[0/25][95/782] Loss_D: 0.6550 Loss_G: 8.3563\n",
            "[0/25][96/782] Loss_D: 0.3901 Loss_G: 7.0715\n",
            "[0/25][97/782] Loss_D: 0.7530 Loss_G: 4.9262\n",
            "[0/25][98/782] Loss_D: 0.4714 Loss_G: 10.2338\n",
            "[0/25][99/782] Loss_D: 0.4668 Loss_G: 8.1141\n",
            "[0/25][100/782] Loss_D: 0.1431 Loss_G: 5.5339\n",
            "[0/25][101/782] Loss_D: 0.7895 Loss_G: 11.8146\n",
            "[0/25][102/782] Loss_D: 0.2961 Loss_G: 9.2589\n",
            "[0/25][103/782] Loss_D: 0.5547 Loss_G: 3.9450\n",
            "[0/25][104/782] Loss_D: 1.6576 Loss_G: 10.4761\n",
            "[0/25][105/782] Loss_D: 1.0286 Loss_G: 7.6917\n",
            "[0/25][106/782] Loss_D: 0.3100 Loss_G: 3.9887\n",
            "[0/25][107/782] Loss_D: 1.0029 Loss_G: 8.1908\n",
            "[0/25][108/782] Loss_D: 0.9095 Loss_G: 5.7021\n",
            "[0/25][109/782] Loss_D: 0.3239 Loss_G: 3.6873\n",
            "[0/25][110/782] Loss_D: 0.6557 Loss_G: 6.5788\n",
            "[0/25][111/782] Loss_D: 0.5350 Loss_G: 5.0077\n",
            "[0/25][112/782] Loss_D: 0.2198 Loss_G: 3.7953\n",
            "[0/25][113/782] Loss_D: 0.4783 Loss_G: 4.9001\n",
            "[0/25][114/782] Loss_D: 0.3897 Loss_G: 3.8327\n",
            "[0/25][115/782] Loss_D: 0.4564 Loss_G: 4.8096\n",
            "[0/25][116/782] Loss_D: 0.4551 Loss_G: 4.5390\n",
            "[0/25][117/782] Loss_D: 0.5469 Loss_G: 4.4811\n",
            "[0/25][118/782] Loss_D: 0.5929 Loss_G: 7.5813\n",
            "[0/25][119/782] Loss_D: 0.6542 Loss_G: 3.8147\n",
            "[0/25][120/782] Loss_D: 0.7218 Loss_G: 8.1844\n",
            "[0/25][121/782] Loss_D: 0.7156 Loss_G: 5.3074\n",
            "[0/25][122/782] Loss_D: 0.2574 Loss_G: 4.6696\n",
            "[0/25][123/782] Loss_D: 0.3633 Loss_G: 6.8552\n",
            "[0/25][124/782] Loss_D: 0.6319 Loss_G: 3.4236\n",
            "[0/25][125/782] Loss_D: 0.4756 Loss_G: 6.2611\n",
            "[0/25][126/782] Loss_D: 0.4683 Loss_G: 4.8047\n",
            "[0/25][127/782] Loss_D: 0.2944 Loss_G: 3.7643\n",
            "[0/25][128/782] Loss_D: 0.4874 Loss_G: 5.7482\n",
            "[0/25][129/782] Loss_D: 0.2047 Loss_G: 5.0905\n",
            "[0/25][130/782] Loss_D: 0.2346 Loss_G: 3.8344\n",
            "[0/25][131/782] Loss_D: 0.2731 Loss_G: 5.4976\n",
            "[0/25][132/782] Loss_D: 0.2527 Loss_G: 4.9945\n",
            "[0/25][133/782] Loss_D: 0.4235 Loss_G: 6.0954\n",
            "[0/25][134/782] Loss_D: 0.3121 Loss_G: 5.2439\n",
            "[0/25][135/782] Loss_D: 0.4155 Loss_G: 5.7393\n",
            "[0/25][136/782] Loss_D: 0.4123 Loss_G: 5.9798\n",
            "[0/25][137/782] Loss_D: 0.3560 Loss_G: 4.5632\n",
            "[0/25][138/782] Loss_D: 0.3533 Loss_G: 6.7069\n",
            "[0/25][139/782] Loss_D: 0.3569 Loss_G: 3.4300\n",
            "[0/25][140/782] Loss_D: 0.5307 Loss_G: 8.9043\n",
            "[0/25][141/782] Loss_D: 1.0217 Loss_G: 3.3568\n",
            "[0/25][142/782] Loss_D: 0.6326 Loss_G: 7.6108\n",
            "[0/25][143/782] Loss_D: 0.5029 Loss_G: 6.3586\n",
            "[0/25][144/782] Loss_D: 0.1396 Loss_G: 4.5001\n",
            "[0/25][145/782] Loss_D: 0.3299 Loss_G: 5.4451\n",
            "[0/25][146/782] Loss_D: 0.2875 Loss_G: 5.2343\n",
            "[0/25][147/782] Loss_D: 0.2205 Loss_G: 4.7855\n",
            "[0/25][148/782] Loss_D: 0.2689 Loss_G: 5.4414\n",
            "[0/25][149/782] Loss_D: 0.2540 Loss_G: 4.5907\n",
            "[0/25][150/782] Loss_D: 0.4372 Loss_G: 6.1792\n",
            "[0/25][151/782] Loss_D: 0.3949 Loss_G: 4.9574\n",
            "[0/25][152/782] Loss_D: 0.2476 Loss_G: 5.7527\n",
            "[0/25][153/782] Loss_D: 0.2702 Loss_G: 7.3911\n",
            "[0/25][154/782] Loss_D: 0.4389 Loss_G: 3.4647\n",
            "[0/25][155/782] Loss_D: 1.0060 Loss_G: 11.8688\n",
            "[0/25][156/782] Loss_D: 2.0364 Loss_G: 7.5254\n",
            "[0/25][157/782] Loss_D: 0.1340 Loss_G: 3.9878\n",
            "[0/25][158/782] Loss_D: 0.8218 Loss_G: 9.7516\n",
            "[0/25][159/782] Loss_D: 1.8655 Loss_G: 5.0172\n",
            "[0/25][160/782] Loss_D: 0.2659 Loss_G: 3.6087\n",
            "[0/25][161/782] Loss_D: 0.6695 Loss_G: 8.3305\n",
            "[0/25][162/782] Loss_D: 0.8642 Loss_G: 5.7089\n",
            "[0/25][163/782] Loss_D: 0.2163 Loss_G: 3.1400\n",
            "[0/25][164/782] Loss_D: 0.9873 Loss_G: 10.3380\n",
            "[0/25][165/782] Loss_D: 0.9092 Loss_G: 8.5707\n",
            "[0/25][166/782] Loss_D: 0.2485 Loss_G: 4.4359\n",
            "[0/25][167/782] Loss_D: 0.5018 Loss_G: 5.9071\n",
            "[0/25][168/782] Loss_D: 0.3617 Loss_G: 6.1443\n",
            "[0/25][169/782] Loss_D: 0.3092 Loss_G: 5.4686\n",
            "[0/25][170/782] Loss_D: 0.2365 Loss_G: 5.5331\n",
            "[0/25][171/782] Loss_D: 0.3217 Loss_G: 5.0513\n",
            "[0/25][172/782] Loss_D: 0.4753 Loss_G: 9.4096\n",
            "[0/25][173/782] Loss_D: 0.8686 Loss_G: 5.3577\n",
            "[0/25][174/782] Loss_D: 0.3527 Loss_G: 5.5413\n",
            "[0/25][175/782] Loss_D: 0.2648 Loss_G: 6.0820\n",
            "[0/25][176/782] Loss_D: 0.2325 Loss_G: 5.5010\n",
            "[0/25][177/782] Loss_D: 0.2361 Loss_G: 5.2657\n",
            "[0/25][178/782] Loss_D: 0.2999 Loss_G: 5.6678\n",
            "[0/25][179/782] Loss_D: 0.2966 Loss_G: 5.5389\n",
            "[0/25][180/782] Loss_D: 0.1936 Loss_G: 5.7041\n",
            "[0/25][181/782] Loss_D: 0.2361 Loss_G: 4.9259\n",
            "[0/25][182/782] Loss_D: 0.2661 Loss_G: 6.6585\n",
            "[0/25][183/782] Loss_D: 0.2574 Loss_G: 5.8999\n",
            "[0/25][184/782] Loss_D: 0.2584 Loss_G: 6.3599\n",
            "[0/25][185/782] Loss_D: 0.2282 Loss_G: 4.9294\n",
            "[0/25][186/782] Loss_D: 0.3729 Loss_G: 9.6645\n",
            "[0/25][187/782] Loss_D: 0.4752 Loss_G: 6.2510\n",
            "[0/25][188/782] Loss_D: 0.1483 Loss_G: 4.8611\n",
            "[0/25][189/782] Loss_D: 0.3030 Loss_G: 8.4830\n",
            "[0/25][190/782] Loss_D: 0.1412 Loss_G: 7.3566\n",
            "[0/25][191/782] Loss_D: 0.1784 Loss_G: 4.7949\n",
            "[0/25][192/782] Loss_D: 0.5568 Loss_G: 9.0027\n",
            "[0/25][193/782] Loss_D: 0.3443 Loss_G: 7.1525\n",
            "[0/25][194/782] Loss_D: 0.1327 Loss_G: 5.7439\n"
          ]
        }
      ]
    }
  ]
}